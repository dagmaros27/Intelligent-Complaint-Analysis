Absolutely! Hereâ€™s a professional, well-structured `README.md` for your project, reflecting the two tasks you have completed.

---

# ğŸ“Š Intelligent Complaint Analysis for Financial Services

**RAG-powered chatbot to turn customer feedback into actionable insights**

---

## ğŸš€ Overview

This project builds an intelligent, Retrieval-Augmented Generation (RAG) powered chatbot for **CrediTrust Financial**, a digital finance company operating across East Africa. The chatbot transforms raw, unstructured customer complaint data into strategic insights. It allows internal teamsâ€”like Product Managers, Compliance, and Supportâ€”to query customer pain points in natural language and receive evidence-backed, concise answers instantly.

---

## ğŸ¯ Business Objectives

- **Reduce time to detect major complaint trends** from days to minutes.
- **Empower non-technical teams** to independently explore customer feedback without data analysts.
- **Shift from reactive to proactive problem solving**, leveraging real-time customer insights.

---

## ğŸ’¡ Key Features

âœ… Supports natural language questions like:

> _â€œWhy are customers unhappy with BNPL?â€_

âœ… Uses **semantic search** (via FAISS / ChromaDB) over complaint narratives.
âœ… Answers questions by grounding LLM responses in retrieved complaints.
âœ… Supports filtering and comparative analysis across multiple products:

- Credit Cards
- Personal Loans
- Buy Now, Pay Later (BNPL)
- Savings Accounts
- Money Transfers

---

## ğŸ“‚ Project Structure

```
project-root/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ filtered_complaints.csv       # Cleaned dataset after Task 1
â”‚
â”œâ”€â”€ vector_store/
â”‚   â””â”€â”€ faiss_index/                  # Vector DB from Task 2
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 1.0-eda.ipynb       # EDA & preprocessing
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ chunking_and_embedding.py         # Text chunking, embedding, indexing
â”‚   â””â”€â”€ utils.py                      # Utility functions
â”‚
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ analysis_report.md            # Summary of EDA findings & design choices
â”‚
â”œâ”€â”€ README.md                         # This file
â””â”€â”€ requirements.txt                  # Python dependencies
```

---

## âœ… Completed Tasks

### ğŸ“ Task 1: Exploratory Data Analysis & Preprocessing

- Loaded CFPB consumer complaints dataset.
- Performed EDA to understand:

  - Complaint distribution across products.
  - Narrative length distributions.
  - Share of complaints with missing narratives.

- Filtered dataset to include only:

  - Five specified products (Credit Cards, Personal Loans, BNPL, Savings Accounts, Money Transfers).
  - Complaints that have non-empty narratives.

- Cleaned narratives:

  - Lowercased text.
  - Removed special characters and boilerplate phrases.

- Saved filtered dataset to:
  â¡ï¸ `data/filtered_complaints.csv`.

### ğŸ§© Task 2: Chunking, Embedding & Vector Store Indexing

- Implemented a text chunking strategy:

  - Used `LangChain`'s `RecursiveCharacterTextSplitter`.
  - Final parameters: `chunk_size=500`, `chunk_overlap=50`.

- Chose `sentence-transformers/all-MiniLM-L6-v2` for embeddings, balancing speed & semantic quality.
- Embedded all text chunks & stored in a FAISS index along with metadata (complaint ID, product).
- Persisted the vector store to:
  â¡ï¸ `vector_store/faiss_index/`.

---

## âš™ï¸ How to Run

### ğŸ”§ Environment Setup

```bash
# Clone the repo
git clone https://github.com/dagmaros27/Intelligent-Complaint-Analysis
cd Intelligent-Complaint-Analysis

# Set up a Python virtual environment
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt
```

### ğŸš€ Running the Pipeline

1. **EDA & Preprocessing**

   ```bash
   jupyter notebook notebooks/1.0-eda.ipynb
   ```

2. **Chunking, Embedding & Indexing**

   ```bash
   python src/chunking_embedding.py
   ```

---

## ğŸ“ Key Learnings So Far

- Handling real-world complaint data is messyâ€”narrative lengths vary hugely, and many complaints are vague.
- Short narratives dominate, but long ones often contain richer contextâ€”highlighting why chunking was critical.
- `all-MiniLM-L6-v2` proved an excellent starting point for balancing semantic relevance with performance on modest hardware.

---

## ğŸ“Œ Next Steps

- Build the **retriever-LLM pipeline** to generate grounded answers.
- Develop a simple **streamlit-based UI** to allow natural language querying.
- Run user simulations to test insights across all five product categories.

---
